from loguru import logger
from pxl_gpt.tokenizer import BPETokenizer

test_sentences = [
    # FranÃ§ais courant
    "Bonjour, comment Ã§a va aujourdâ€™hui ?",
    "Je vais Ã  la bibliothÃ¨que pour emprunter un livre.",
    "Câ€™est incroyable ! Tu as vraiment rÃ©ussi ton projet.",

    # Mots avec accents et caractÃ¨res spÃ©ciaux
    "FranÃ§ois aime le cafÃ© au lait et les croissants.",
    "La naÃ¯vetÃ© est parfois une force, pas une faiblesse.",
    "Lâ€™Ã©lÃ¨ve Ã©tudie lâ€™histoire de lâ€™ÃŽle-de-France.",

    # Mots inconnus ou composÃ©s
    "Anticonstitutionnellement, câ€™est le mot le plus long.",
    "Supercalifragilisticexpialidocious est un mot inventÃ©.",
    "PyTorch et Transformers sont des bibliothÃ¨ques Python utiles.",

    # Chiffres, ponctuation et symboles
    "Mon numÃ©ro est le 06-12-34-56-78 !",
    "Prix : 12,99 â‚¬ â€“ rÃ©duction de 20 %.",
    "Email : exemple@test.com, site web : https://www.example.org",

    # Emojis et symboles Unicode
    "Jâ€™adore ðŸ˜„, mais parfois ðŸ˜¢.",
    "Les maths : âˆ‘, âˆ«, âˆš, Ï€ sont fascinants.",
    "Le code peut Ãªtre ðŸ’» ou ðŸ¤– selon le projet."
]


def main():
    tokenizer = BPETokenizer()
    tokenizer.load("data/tokenizer.json")

    logger.info("Testing tokenizer...")
    for sentence in test_sentences:
        print("=====")
        print(f"Test pour la phrase : '{sentence}'")
        encoded_text = tokenizer.encode(sentence)
        logger.info(f"Encoded : {encoded_text}")

        decoded_text = tokenizer.decode(encoded_text)
        logger.info(f"Decoded text: {decoded_text}")




if __name__ == "__main__":
    main()
